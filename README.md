# Human-AI 협업 프로젝트
## 프로젝트 개요
- YOLO의 이미지 분류와 LSTM 모델을 사용하여 한 손으로 장난감 블록을 조립하는 과정을 보조하는 AI 시스템입니다.

## 📎 파일 소개
- mediapipe와 OpenCV를 사용하여 실시간으로 손의 움직임을 추적하고, 손이 쥐어지거나 펴지는 상태를 감지하여 화면에 표시합니다. 
- 손(오른손)이 쥐어지거나 펴지는 상태를 "Grab" 또는 "Release"로 표시하며, 손의 랜드마크도 시각적으로 보여줍니다.
- YOLO를 사용해 장난감 블럭을 인식하고, 자동차 모양이 조립되는 각 단계가 정확히 수행되는지 판단합니다.

## 🔄 브랜치 변경 안내
- 이 프로젝트의 기능 개발은 주로 feature 브랜치에서 이루어집니다.
- feature 브랜치에서 작업을 시작하려면, 다음의 Git 명령어를 사용하여 브랜치를 변경해 주세요.
`git checkout feature`

## 🔌 설치 방법
- 이 프로젝트를 실행하기 위해서는 Python이 설치되어 있어야 하며, 몇 가지 Python 라이브러리가 필요합니다.
- 필요한 라이브러리는 requirements.txt 파일에 나열되어 있습니다.<br></br>

## ⚡️ 실행 단계
- 설치가 완료되면, 다음과 같이 스크립트를 실행할 수 있습니다.
`python hand_tracking.py`
- 실행 후, 카메라가 활성화되고 손의 움직임을 추적하기 시작합니다.
- 프로그램은 손의 상태를 "Grab" 또는 "Release"로 표시하며, 손의 랜드마크를 화면에 그립니다.
- YOLO를 통해 감지된 장난감 블록 주변에 경계 상자를 그리고, 사용자가 해당 블록을 집어 올리라는 텍스트 프롬프트를 제공합니다.
- LSTM 모델을 사용하여 손이 블록을 올바르게 집어 들었는지 감지합니다. 모델이 손이 블록을 집었다고 판단하면 ('grab' 상태), 프롬프트가 사용자에게 블록을 주어진 위치에 놓으라고 지시합니다.
- 블록이 놓여지고 ('release' 상태), YOLO 모델이 블록이 올바르게 조합되었다고 인식하면, 텍스트 프롬프트가 피드백 ('참 잘했어요!')을 제공하고, 다음 단계로 넘어갑니다(사용해야 할 다음 블록 주위에 경계 상자를 그려주고 지시 텍스트 프롬프트를 제공합니다).
- 최종 모형이 완성될 때까지 반복됩니다.

## 👋 종료 방법
- 프로그램을 종료하려면, 'q' 키를 누릅니다.
